<!DOCTYPE html>
<html lang=zh>
<head><meta name="generator" content="Hexo 3.8.0">
    <!-- so meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <meta name="description" content="1.简介KPU 是通用神经网络处理器，内置卷积、批归一化、激活、池化运算单元，可以对人脸或物体进行实时检测，具体特性如下：  支持主流训练框架按照特定限制规则训练出来的定点化模型 对网络层数无直接限制，支持每层卷积神经网络参数单独配置，包括输入输出通道数目、输入输出行宽列高 支持两种卷积内核 1x1 和 3x3 支持任意形式的激活函数 实时工作时最大支持神经网络参数大小为 5.5MiB 到 5.9">
<meta name="keywords" content="MCU,k210,KPU">
<meta property="og:type" content="article">
<meta property="og:title" content="kendryte kpu">
<meta property="og:url" content="https://elfmedy.github.io/2019/04/18/kendryte-kpu/index.html">
<meta property="og:site_name" content="elfmedy">
<meta property="og:description" content="1.简介KPU 是通用神经网络处理器，内置卷积、批归一化、激活、池化运算单元，可以对人脸或物体进行实时检测，具体特性如下：  支持主流训练框架按照特定限制规则训练出来的定点化模型 对网络层数无直接限制，支持每层卷积神经网络参数单独配置，包括输入输出通道数目、输入输出行宽列高 支持两种卷积内核 1x1 和 3x3 支持任意形式的激活函数 实时工作时最大支持神经网络参数大小为 5.5MiB 到 5.9">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="https://elfmedy.github.io/2019/04/18/kendryte-kpu/kpu1.png">
<meta property="og:image" content="https://elfmedy.github.io/2019/04/18/kendryte-kpu/kpu2.jpg">
<meta property="og:updated_time" content="2020-01-20T02:11:08.101Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="kendryte kpu">
<meta name="twitter:description" content="1.简介KPU 是通用神经网络处理器，内置卷积、批归一化、激活、池化运算单元，可以对人脸或物体进行实时检测，具体特性如下：  支持主流训练框架按照特定限制规则训练出来的定点化模型 对网络层数无直接限制，支持每层卷积神经网络参数单独配置，包括输入输出通道数目、输入输出行宽列高 支持两种卷积内核 1x1 和 3x3 支持任意形式的激活函数 实时工作时最大支持神经网络参数大小为 5.5MiB 到 5.9">
<meta name="twitter:image" content="https://elfmedy.github.io/2019/04/18/kendryte-kpu/kpu1.png">
    
    
        
          
              <link rel="shortcut icon" href="/images/favicon.ico">
          
        
        
          
            <link rel="icon" type="image/png" href="/images/favicon-192x192.png" sizes="192x192">
          
        
        
          
            <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
          
        
    
    <!-- title -->
    <title>kendryte kpu</title>
    <!-- styles -->
    <link rel="stylesheet" href="/css/style.css">
    <!-- persian styles -->
    
      <link rel="stylesheet" href="/css/rtl.css">
    
    <!-- rss -->
    
    
</head>

<body class="max-width mx-auto px3 ltr">
    
      <div id="header-post">
  <a id="menu-icon" href="#"><i class="fas fa-bars fa-lg"></i></a>
  <a id="menu-icon-tablet" href="#"><i class="fas fa-bars fa-lg"></i></a>
  <a id="top-icon-tablet" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" style="display:none;"><i class="fas fa-chevron-up fa-lg"></i></a>
  <span id="menu">
    <span id="nav">
      <ul>
         
          <li><a href="/">首页</a></li>
         
          <li><a href="/about/">关于</a></li>
         
          <li><a href="/archives/">归档</a></li>
         
          <li><a href="/projects_url">项目</a></li>
        
      </ul>
    </span>
    <br>
    <span id="actions">
      <ul>
        
        <li><a class="icon" href="/2019/04/18/kendryte 外设/"><i class="fas fa-chevron-left" aria-hidden="true" onmouseover="$('#i-prev').toggle();" onmouseout="$('#i-prev').toggle();"></i></a></li>
        
        
        <li><a class="icon" href="/2019/04/18/kendryte dvp/"><i class="fas fa-chevron-right" aria-hidden="true" onmouseover="$('#i-next').toggle();" onmouseout="$('#i-next').toggle();"></i></a></li>
        
        <li><a class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up" aria-hidden="true" onmouseover="$('#i-top').toggle();" onmouseout="$('#i-top').toggle();"></i></a></li>
        <li><a class="icon" href="#"><i class="fas fa-share-alt" aria-hidden="true" onmouseover="$('#i-share').toggle();" onmouseout="$('#i-share').toggle();" onclick="$('#share').toggle();return false;"></i></a></li>
      </ul>
      <span id="i-prev" class="info" style="display:none;">上一篇</span>
      <span id="i-next" class="info" style="display:none;">下一篇</span>
      <span id="i-top" class="info" style="display:none;">返回顶部</span>
      <span id="i-share" class="info" style="display:none;">分享文章</span>
    </span>
    <br>
    <div id="share" style="display: none">
      <ul>
  <li><a class="icon" href="http://www.facebook.com/sharer.php?u=https://elfmedy.github.io/2019/04/18/kendryte-kpu/"><i class="fab fa-facebook " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://twitter.com/share?url=https://elfmedy.github.io/2019/04/18/kendryte-kpu/&text=kendryte kpu"><i class="fab fa-twitter " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.linkedin.com/shareArticle?url=https://elfmedy.github.io/2019/04/18/kendryte-kpu/&title=kendryte kpu"><i class="fab fa-linkedin " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://pinterest.com/pin/create/bookmarklet/?url=https://elfmedy.github.io/2019/04/18/kendryte-kpu/&is_video=false&description=kendryte kpu"><i class="fab fa-pinterest " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=kendryte kpu&body=Check out this article: https://elfmedy.github.io/2019/04/18/kendryte-kpu/"><i class="fas fa-envelope " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://getpocket.com/save?url=https://elfmedy.github.io/2019/04/18/kendryte-kpu/&title=kendryte kpu"><i class="fab fa-get-pocket " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://reddit.com/submit?url=https://elfmedy.github.io/2019/04/18/kendryte-kpu/&title=kendryte kpu"><i class="fab fa-reddit " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.stumbleupon.com/submit?url=https://elfmedy.github.io/2019/04/18/kendryte-kpu/&title=kendryte kpu"><i class="fab fa-stumbleupon " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://digg.com/submit?url=https://elfmedy.github.io/2019/04/18/kendryte-kpu/&title=kendryte kpu"><i class="fab fa-digg " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.tumblr.com/share/link?url=https://elfmedy.github.io/2019/04/18/kendryte-kpu/&name=kendryte kpu&description="><i class="fab fa-tumblr " aria-hidden="true"></i></a></li>
</ul>

    </div>
    <div id="toc">
      <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-简介"><span class="toc-number">1.</span> <span class="toc-text">1.简介</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-工作原理"><span class="toc-number">2.</span> <span class="toc-text">2.工作原理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-代码实现（加载-gencode-output-c文件）"><span class="toc-number">3.</span> <span class="toc-text">3.代码实现（加载 gencode_output.c文件）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-代码实现（加载-kmodel）"><span class="toc-number">4.</span> <span class="toc-text">4.代码实现（加载 .kmodel）</span></a></li></ol>
    </div>
  </span>
</div>

    
    <div class="content index py4">
        
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">
  <header>
    
    <h1 class="posttitle" itemprop="name headline">
        kendryte kpu
    </h1>



    <div class="meta">
      <span class="author" itemprop="author" itemscope itemtype="http://schema.org/Person">
        <span itemprop="name">elfmedy</span>
      </span>
      发布于
      
    <div class="postdate">
      
        <time datetime="2019-04-18T14:11:41.000Z" itemprop="datePublished">2019-04-18</time>
        
      
    </div>


      

      
    <div class="article-tag">
        tag:
        <a class="tag-link" href="/tags/KPU/">KPU</a>, <a class="tag-link" href="/tags/MCU/">MCU</a>, <a class="tag-link" href="/tags/k210/">k210</a>
    </div>


    </div>
  </header>
  

  <div class="content" itemprop="articleBody">
    <h2 id="1-简介"><a href="#1-简介" class="headerlink" title="1.简介"></a>1.简介</h2><p>KPU 是通用神经网络处理器，内置卷积、批归一化、激活、池化运算单元，可以对人脸或物体进行实时检测，具体特性如下：</p>
<ul>
<li>支持主流训练框架按照特定限制规则训练出来的定点化模型</li>
<li>对网络层数无直接限制，支持每层卷积神经网络参数单独配置，包括输入输出通道数目、输入输出行宽列高</li>
<li>支持两种卷积内核 1x1 和 3x3</li>
<li>支持任意形式的激活函数</li>
<li>实时工作时最大支持神经网络参数大小为 5.5MiB 到 5.9MiB</li>
<li>非实时工作时最大支持网络参数大小为（Flash 容量-软件体积）</li>
</ul>
<img src="/2019/04/18/kendryte-kpu/kpu1.png" title="kpu ram">
<ol>
<li>*非实时场合一般用于音频应用，这类应用一般不需要 33ms 内获得神经网络输出结果。</li>
<li>*Flash 大小可选择为：SPI NOR Flash（8MiB，16MiB，32MiB），SPI NAND Flash（64MiB，128MiB，256MiB），用户可根据需要选择合适的 Flash</li>
</ol>
<p>注意，KPU 只支持卷积相关的操作，所以其他的全连接什么的都是需要 CPU 来实现的，sdk 中也有相关的代码实现。</p>
<h2 id="2-工作原理"><a href="#2-工作原理" class="headerlink" title="2.工作原理"></a>2.工作原理</h2><img src="/2019/04/18/kendryte-kpu/kpu2.jpg" title="kpu">
<p>引用 &lt;<a href="https://bbs.sipeed.com/t/topic/502" target="_blank" rel="noopener">https://bbs.sipeed.com/t/topic/502</a>&gt;</p>
<h2 id="3-代码实现（加载-gencode-output-c文件）"><a href="#3-代码实现（加载-gencode-output-c文件）" class="headerlink" title="3.代码实现（加载 gencode_output.c文件）"></a>3.代码实现（加载 gencode_output.c文件）</h2><p>需要说明的是，官方目前的 sdk develop 分支中，已经不再采用 gencode_output.c 文件来初始化了，而是使用 .kmodel 模型来初始化 kpu，而且 sdk 的代码也做了不少修改，之前的这种方式的问题应该是只能实现卷积相关的操作，而采用 .kmodle 中的结构方式之后，方便使用 CPU 来实现其他的自定义层的操作。</p>
<p>通过 kendryte-model-compile 将模型转换为 k210 支持的形式，这里是 gencode_output.c 文件，其中包含了神经网络的层级结构的结构体和数组（模型的主要占用空间的就是各种权重参数的数组），还有针对这个模型的 kpu_task_init 函数实现。部分代码如下：</p>
<pre class="themepre">
<span class="themespan">// 这个是网络层次结构体，其中包含了各层的参数设置</span>
<span class="themespan">// 因为是数组，所有可以有很多层</span>
kpu_layer_argument_t la[] __attribute__((aligned(128))) = {
// 0
{
 .interrupt_enabe.data = {
  .int_en = 0,
  .ram_flag = 0,
  .full_add = 0,
  .depth_wise_layer = 0
 },
 .image_addr.data = {
  .image_src_addr = (uint64_t)0x0,
  .image_dst_addr = (uint64_t)0x6980
 },
 .image_channel_num.data = {
  .i_ch_num = 0x2,
  .o_ch_num = 0xf,
  .o_ch_num_coef = 0xf
 },
 .image_size.data = {
  .i_row_wid = 0x13f,
  .i_col_high = 0xef,
  .o_row_wid = 0x9f,
  .o_col_high = 0x77
 },
 .kernel_pool_type_cfg.data = {
  .kernel_type = 1,
  .pad_type = 0,
  .pool_type = 1,
  .first_stride = 0,
  .bypass_conv = 0,
  .load_para = 1,
  .dma_burst_size = 15,
  .pad_value = 0x0,
  .bwsx_base_addr = 0
 },
 .kernel_load_cfg.data = {
  .load_coor = 1,
  .load_time = 0,
  .para_size = 864,
  .para_start_addr = 0
 },
 .kernel_offset.data = {
  .coef_column_offset = 0,
  .coef_row_offset = 0
 },
 .kernel_calc_type_cfg.data = {
  .channel_switch_addr = 0x4b0,
  .row_switch_addr = 0x5,
  .coef_size = 0,
  .coef_group = 1,
  .load_act = 1,
  .active_addr = 0
 },
 .write_back_cfg.data = {
  .wb_channel_switch_addr = 0x168,
  .wb_row_switch_addr = 0x3,
  .wb_group = 1
 },
 .conv_value.data = {
  .shr_w = 0,
  .shr_x = 7,
  .arg_w = 0x0,
  .arg_x = 0xb8cb99
 },
 .conv_value2.data = {
  .arg_add = 0
 },
 .dma_parameter.data = {
  .send_data_out = 0,
  .channel_byte_num = 19199,
  .dma_total_byte = 307199
 }
},
// 1
{
 .interrupt_enabe.data = {
  .int_en = 0,

<span class="themespan">// kpu_task_init 函数</span>
<span class="themespan">// 除了赋值 layer 函数之外，还对 layer 里面的每一层里面的数据指针做了赋值（层结构体中的指针指向权重系数等）</span>
kpu_task_t* kpu_task_init(kpu_task_t* task){
 la[0].kernel_pool_type_cfg.data.bwsx_base_addr = (uint64_t)&amp;bwsx_base_addr_0;  <span class="themespan">// 这些都是指向系数的数组</span>
 la[0].kernel_calc_type_cfg.data.active_addr = (uint64_t)&amp;active_addr_0;
 la[0].kernel_load_cfg.data.para_start_addr = (uint64_t)&amp;para_start_addr_0; 
 la[1].kernel_pool_type_cfg.data.bwsx_base_addr = (uint64_t)&amp;bwsx_base_addr_1;
 la[1].kernel_calc_type_cfg.data.active_addr = (uint64_t)&amp;active_addr_1;
 la[1].kernel_load_cfg.data.para_start_addr = (uint64_t)&amp;para_start_addr_1; 
 la[2].kernel_pool_type_cfg.data.bwsx_base_addr = (uint64_t)&amp;bwsx_base_addr_2;
 la[2].kernel_calc_type_cfg.data.active_addr = (uint64_t)&amp;active_addr_2;
 la[2].kernel_load_cfg.data.para_start_addr = (uint64_t)&amp;para_start_addr_2; 
 la[3].kernel_pool_type_cfg.data.bwsx_base_addr = (uint64_t)&amp;bwsx_base_addr_3;
 la[3].kernel_calc_type_cfg.data.active_addr = (uint64_t)&amp;active_addr_3;
 la[3].kernel_load_cfg.data.para_start_addr = (uint64_t)&amp;para_start_addr_3; 
 la[4].kernel_pool_type_cfg.data.bwsx_base_addr = (uint64_t)&amp;bwsx_base_addr_4;
 la[4].kernel_calc_type_cfg.data.active_addr = (uint64_t)&amp;active_addr_4;
 la[4].kernel_load_cfg.data.para_start_addr = (uint64_t)&amp;para_start_addr_4; 
 la[5].kernel_pool_type_cfg.data.bwsx_base_addr = (uint64_t)&amp;bwsx_base_addr_5;
 la[5].kernel_calc_type_cfg.data.active_addr = (uint64_t)&amp;active_addr_5;
 la[5].kernel_load_cfg.data.para_start_addr = (uint64_t)&amp;para_start_addr_5; 
 la[6].kernel_pool_type_cfg.data.bwsx_base_addr = (uint64_t)&amp;bwsx_base_addr_6;
 la[6].kernel_calc_type_cfg.data.active_addr = (uint64_t)&amp;active_addr_6;
 la[6].kernel_load_cfg.data.para_start_addr = (uint64_t)&amp;para_start_addr_6; 
 la[7].kernel_pool_type_cfg.data.bwsx_base_addr = (uint64_t)&amp;bwsx_base_addr_7;
 la[7].kernel_calc_type_cfg.data.active_addr = (uint64_t)&amp;active_addr_7;
 la[7].kernel_load_cfg.data.para_start_addr = (uint64_t)&amp;para_start_addr_7; 
 la[8].kernel_pool_type_cfg.data.bwsx_base_addr = (uint64_t)&amp;bwsx_base_addr_8;
 la[8].kernel_calc_type_cfg.data.active_addr = (uint64_t)&amp;active_addr_8;
 la[8].kernel_load_cfg.data.para_start_addr = (uint64_t)&amp;para_start_addr_8; 
 la[9].kernel_pool_type_cfg.data.bwsx_base_addr = (uint64_t)&amp;bwsx_base_addr_9;
 la[9].kernel_calc_type_cfg.data.active_addr = (uint64_t)&amp;active_addr_9;
 la[9].kernel_load_cfg.data.para_start_addr = (uint64_t)&amp;para_start_addr_9; 
 la[10].kernel_pool_type_cfg.data.bwsx_base_addr = (uint64_t)&amp;bwsx_base_addr_10;
 la[10].kernel_calc_type_cfg.data.active_addr = (uint64_t)&amp;active_addr_10;
 la[10].kernel_load_cfg.data.para_start_addr = (uint64_t)&amp;para_start_addr_10; 
 la[11].kernel_pool_type_cfg.data.bwsx_base_addr = (uint64_t)&amp;bwsx_base_addr_11;
 la[11].kernel_calc_type_cfg.data.active_addr = (uint64_t)&amp;active_addr_11;
 la[11].kernel_load_cfg.data.para_start_addr = (uint64_t)&amp;para_start_addr_11; 
 la[12].kernel_pool_type_cfg.data.bwsx_base_addr = (uint64_t)&amp;bwsx_base_addr_12;
 la[12].kernel_calc_type_cfg.data.active_addr = (uint64_t)&amp;active_addr_12;
 la[12].kernel_load_cfg.data.para_start_addr = (uint64_t)&amp;para_start_addr_12; 
 la[13].kernel_pool_type_cfg.data.bwsx_base_addr = (uint64_t)&amp;bwsx_base_addr_13;
 la[13].kernel_calc_type_cfg.data.active_addr = (uint64_t)&amp;active_addr_13;
 la[13].kernel_load_cfg.data.para_start_addr = (uint64_t)&amp;para_start_addr_13; 
 la[14].kernel_pool_type_cfg.data.bwsx_base_addr = (uint64_t)&amp;bwsx_base_addr_14;
 la[14].kernel_calc_type_cfg.data.active_addr = (uint64_t)&amp;active_addr_14;
 la[14].kernel_load_cfg.data.para_start_addr = (uint64_t)&amp;para_start_addr_14; 
 la[15].kernel_pool_type_cfg.data.bwsx_base_addr = (uint64_t)&amp;bwsx_base_addr_15;
 la[15].kernel_calc_type_cfg.data.active_addr = (uint64_t)&amp;active_addr_15;
 la[15].kernel_load_cfg.data.para_start_addr = (uint64_t)&amp;para_start_addr_15;
 task-&gt;layers = la;
 task-&gt;layers_length = sizeof(la)/sizeof(la[0]);
 task-&gt;eight_bit_mode = 0;
 task-&gt;output_scale = 0.06265033647125842;
 task-&gt;output_bias = -14.453129768371582;
 return task;
}
</pre>

<p>至于 kpu 的运行，主要体现在 kpu_run 函数中，部分代码如下。先使用 DMA 将输入图像数据复制到 KPU 的处理的物理地址，然后根据 KPU 的中断将网络参数赋值给 KPU 的网络参数 fifo 寄存器（这里是每次写入一层网络参数）。在 KPU 计算完成之后触发 DMA 中断将数据拷贝到输出缓存。</p>
<p>KPU 的硬件寄存器接收的参数实际就是一组 uint64 的数据，每次循环写入 kpu-&gt;layer_argument_fifo，这个是固定的。而 task 结构体恰好设置为和 KPU 的硬件寄存器接收的数据格式一致。</p>
<pre class="themepre">
<span class="themespan">// kpu 中断处理函数</span>
int kpu_continue(void* _task)
{
    kpu_task_t* task = (kpu_task_t*)_task;
    int layer_burst_size = 1;

    kpu-&gt;interrupt_clear.data = (kpu_config_interrupt_t)
    {
        .calc_done_int=1,
        .layer_cfg_almost_empty_int=1,
        .layer_cfg_almost_full_int=1
    };

    if(task-&gt;remain_layers_length == 0)
    {
        return 0;
    }
    if(task-&gt;remain_layers_length &lt;= layer_burst_size)
    {
        for(uint32_t i=0; i&lt;task-&gt;remain_layers_length; i++)
        {
            <span class="themespan">// 这里是重复写 fifo 寄存器，直到所有参数写完</span>
            kpu-&gt;layer_argument_fifo = task-&gt;remain_layers[i].interrupt_enabe.reg;
            kpu-&gt;layer_argument_fifo = task-&gt;remain_layers[i].image_addr.reg;
            kpu-&gt;layer_argument_fifo = task-&gt;remain_layers[i].image_channel_num.reg;
            kpu-&gt;layer_argument_fifo = task-&gt;remain_layers[i].image_size.reg;
            kpu-&gt;layer_argument_fifo = task-&gt;remain_layers[i].kernel_pool_type_cfg.reg;
            kpu-&gt;layer_argument_fifo = task-&gt;remain_layers[i].kernel_load_cfg.reg;
            kpu-&gt;layer_argument_fifo = task-&gt;remain_layers[i].kernel_offset.reg;
            kpu-&gt;layer_argument_fifo = task-&gt;remain_layers[i].kernel_calc_type_cfg.reg;
            kpu-&gt;layer_argument_fifo = task-&gt;remain_layers[i].write_back_cfg.reg;
            kpu-&gt;layer_argument_fifo = task-&gt;remain_layers[i].conv_value.reg;
            kpu-&gt;layer_argument_fifo = task-&gt;remain_layers[i].conv_value2.reg;
            kpu-&gt;layer_argument_fifo = task-&gt;remain_layers[i].dma_parameter.reg;
        }
        task-&gt;remain_layers_length = 0;
    }
    else
    {
        <span class="themespan">// 在中断处理函数中依次将每一层的网络参数赋值给 kpu 寄存器的 layer_argument_fifo 中</span>
        for(uint32_t i=0; i&lt;layer_burst_size; i++)
        {
            kpu-&gt;layer_argument_fifo = task-&gt;remain_layers[i].interrupt_enabe.reg;
            kpu-&gt;layer_argument_fifo = task-&gt;remain_layers[i].image_addr.reg;
            kpu-&gt;layer_argument_fifo = task-&gt;remain_layers[i].image_channel_num.reg;
            kpu-&gt;layer_argument_fifo = task-&gt;remain_layers[i].image_size.reg;
            kpu-&gt;layer_argument_fifo = task-&gt;remain_layers[i].kernel_pool_type_cfg.reg;
            kpu-&gt;layer_argument_fifo = task-&gt;remain_layers[i].kernel_load_cfg.reg;
            kpu-&gt;layer_argument_fifo = task-&gt;remain_layers[i].kernel_offset.reg;
            kpu-&gt;layer_argument_fifo = task-&gt;remain_layers[i].kernel_calc_type_cfg.reg;
            kpu-&gt;layer_argument_fifo = task-&gt;remain_layers[i].write_back_cfg.reg;
            kpu-&gt;layer_argument_fifo = task-&gt;remain_layers[i].conv_value.reg;
            kpu-&gt;layer_argument_fifo = task-&gt;remain_layers[i].conv_value2.reg;
            kpu-&gt;layer_argument_fifo = task-&gt;remain_layers[i].dma_parameter.reg;
        }
        task-&gt;remain_layers += layer_burst_size;
        task-&gt;remain_layers_length -= layer_burst_size;
    }
    printf(&quot;kpu_continue. remain_layers_length=%d\n&quot;, task-&gt;remain_layers_length);

    return 0;
}

<span class="themespan">// 将数据放入 dma 处理地址地址之后的回调函数</span>
static int kpu_run_dma_input_done_push_layers(void* _task)
{
    printf(&quot;kpu_run_dma_input_done_push_layers. input dma over cb\n&quot;);

    kpu_task_t* task = (kpu_task_t*)_task;
    kpu-&gt;interrupt_clear.reg = 7;
    dmac-&gt;channel[task-&gt;dma_ch].intclear = 0xFFFFFFFF;
    kpu-&gt;fifo_threshold.data = (kpu_config_fifo_threshold_t)
    {
        .fifo_full_threshold = 10, .fifo_empty_threshold=1
    };
    kpu-&gt;eight_bit_mode.data = (kpu_config_eight_bit_mode_t)
    {
        .eight_bit_mode=task-&gt;eight_bit_mode
    };

    kpu_layer_argument_t* last_layer = &amp;task-&gt;layers[task-&gt;layers_length-1];

    <span class="themespan">// 这里只是注册 kpu 运算结束之后的 dma 操作。在运算结束之后将输出数据复制到输出缓冲区中</span>
    <span class="themespan">// 这个函数中会调用 sysctl_dma_select(dma_ch, SYSCTL_DMA_SELECT_AI_RX_REQ); 这个选择的中断源应该就是 kpu 计算结束之后的 RX 请求</span>
    kpu_run_dma_output(task-&gt;dma_ch, task-&gt;dst, last_layer-&gt;dma_parameter.data.dma_total_byte+1, kpu_run_all_done, task);

    kpu-&gt;interrupt_mask.data = (kpu_config_interrupt_t)
    {
        .calc_done_int=0,
        .layer_cfg_almost_empty_int=0,
        .layer_cfg_almost_full_int=1
    };

    <span class="themespan">// 这里开始会放入一次，后面调用 kpu_continue 就都是 kpu 的中断本身在调用了</span>
    kpu_continue(task);
    return 0;
}

<span class="themespan">// kpu_run 函数</span>
int kpu_run(kpu_task_t* v_task, dmac_channel_number_t dma_ch, const void *src, void* dest, plic_irq_callback_t callback)
{
    if(atomic_cas(&amp;g_kpu_context.kpu_status, 0, 1))
        return -1;

    memcpy((void *)&amp;g_kpu_context.kpu_task, v_task, sizeof(kpu_task_t));
    kpu_task_t *task = (kpu_task_t *)&amp;g_kpu_context.kpu_task;

    kpu_layer_argument_t* last_layer = &amp;task-&gt;layers[task-&gt;layers_length-1];

    uint64_t output_size = last_layer-&gt;dma_parameter.data.dma_total_byte+1;
    uint32_t size = output_size;

    last_layer-&gt;dma_parameter.data.send_data_out = 1;
    last_layer-&gt;interrupt_enabe.data.int_en = 1;

    task-&gt;dma_ch = dma_ch;
    task-&gt;dst = dest;
    task-&gt;dst_length = output_size;
    task-&gt;cb = callback;
    task-&gt;remain_layers_length = task-&gt;layers_length;
    task-&gt;remain_layers = task-&gt;layers;

    <span class="themespan">// 使能 kpu 的中断处理函数</span>
    plic_irq_enable(IRQN_AI_INTERRUPT);
    plic_set_priority(IRQN_AI_INTERRUPT, 1);
    plic_irq_register(IRQN_AI_INTERRUPT, kpu_continue, task);

    <span class="themespan">// 将输入图像数据放入 kpu 的处理地址，src-&gt;AI_IO_BASE_ADDR，这个是立即执行的</span>
    kpu_run_dma_input(dma_ch, src, kpu_run_dma_input_done_push_layers, task);

    return 0;
}
</pre>

<h2 id="4-代码实现（加载-kmodel）"><a href="#4-代码实现（加载-kmodel）" class="headerlink" title="4.代码实现（加载 .kmodel）"></a>4.代码实现（加载 .kmodel）</h2><p>.kmodle 模型可以存放在 flash 中，或者直接使用 sdk 提供的宏来包含模型，也就是将 .kmodel 当做数组。加载模型也就是赋值一些 ctx 的参数。.kmodel 模型是使用 nncase 生成的。</p>
<pre class="themepre">
int kpu_load_kmodel(kpu_model_context_t *ctx, const uint8_t *buffer)
{
    uintptr_t base_addr = (uintptr_t)buffer;
    const kpu_kmodel_header_t *header = (const kpu_kmodel_header_t *)buffer;

    if (header-&gt;version == 3 &amp;&amp; header-&gt;arch == 0)
    {
        ctx-&gt;model_buffer = buffer;
        ctx-&gt;output_count = header-&gt;output_count;
        ctx-&gt;outputs = (const kpu_model_output_t *)(base_addr + sizeof(kpu_kmodel_header_t));
        ctx-&gt;layer_headers = (const kpu_model_layer_header_t *)((uintptr_t)ctx-&gt;outputs + sizeof(kpu_model_output_t) * ctx-&gt;output_count);
        ctx-&gt;layers_length = header-&gt;layers_length;
        ctx-&gt;body_start = (const uint8_t *)((uintptr_t)ctx-&gt;layer_headers + sizeof(kpu_model_layer_header_t) * header-&gt;layers_length);
        ctx-&gt;main_buffer = (uint8_t *)malloc(header-&gt;main_mem_usage);
        if (!ctx-&gt;main_buffer)
            return -1;
    }
    else
    {
        return -1;
    }

    return 0;
}
</pre>

<p>模型运行的话基本和前面使用 gencode_output.c 文件的方式类似，都是一层一层加载网络，不过多出来的地方是增加了使用 CPU 来实现一些操作，下面的一些 cnt_layer_header-&gt;type 判断中，只有卷积相关是直接 DMA 拷贝实现的，其他的都是 CPU 直接算。</p>
<pre class="themepre">
static int ai_step(void *userdata)
{
    kpu_model_context_t *ctx = (kpu_model_context_t *)userdata;

    uint32_t cnt_layer_id = ctx-&gt;current_layer++;
    const uint8_t *layer_body = ctx-&gt;current_body;
    const kpu_model_layer_header_t *cnt_layer_header = ctx-&gt;layer_headers + cnt_layer_id;
    ctx-&gt;current_body += cnt_layer_header-&gt;body_size;

    <span class="themespan">// 除了卷积相关，都是 CPU 实现的</span>
    switch (cnt_layer_header-&gt;type)
    {
        case KL_ADD:
            kpu_kmodel_add((const kpu_model_add_layer_argument_t *)layer_body, ctx);
            break;
        case KL_QUANTIZED_ADD:
            kpu_quantized_add((const kpu_model_quant_add_layer_argument_t *)layer_body, ctx);
            break;
        case KL_GLOBAL_AVERAGE_POOL2D:
            kpu_global_average_pool2d((const kpu_model_gap2d_layer_argument_t *)layer_body, ctx);
            break;
        case KL_QUANTIZED_MAX_POOL2D:
            kpu_quantized_max_pool2d((const kpu_model_quant_max_pool2d_layer_argument_t *)layer_body, ctx);
            break;
        case KL_AVERAGE_POOL2D:
            kpu_average_pool2d((const kpu_model_ave_pool2d_layer_argument_t *)layer_body, ctx);
            break;
        case KL_QUANTIZE:
            kpu_quantize((const kpu_model_quantize_layer_argument_t *)layer_body, ctx);
            break;
        case KL_DEQUANTIZE:
            kpu_kmodel_dequantize((const kpu_model_dequantize_layer_argument_t *)layer_body, ctx);
            break;
        case KL_REQUANTIZE:
            kpu_requantize((const kpu_model_requantize_layer_argument_t *)layer_body, ctx);
            break;
        case KL_L2_NORMALIZATION:
            kpu_l2_normalization((const kpu_model_l2_norm_layer_argument_t *)layer_body, ctx);
            break;
        case KL_SOFTMAX:
            kpu_softmax((const kpu_model_softmax_layer_argument_t *)layer_body, ctx);
            break;
        case KL_CONCAT:
        case KL_QUANTIZED_CONCAT:
            kpu_concat((const kpu_model_concat_layer_argument_t *)layer_body, ctx);
            break;
        case KL_FULLY_CONNECTED:
            kpu_kmodel_fully_connected((const kpu_model_fully_connected_layer_argument_t *)layer_body, ctx);
            break;
        case KL_TENSORFLOW_FLATTEN:
            kpu_tf_flatten((const kpu_model_tf_flatten_layer_argument_t *)layer_body, ctx);
            break;
        case KL_K210_CONV:
            kpu_conv((const kpu_model_conv_layer_argument_t *)layer_body, ctx);
            return 0;
        case KL_K210_ADD_PADDING:
            kpu_add_padding((const kpu_model_add_padding_layer_argument_t *)layer_body, ctx);
            break;
        case KL_K210_REMOVE_PADDING:
            kpu_remove_padding((const kpu_model_remove_padding_layer_argument_t *)layer_body, ctx);
            break;
        case KL_K210_UPLOAD:
            kpu_upload((const kpu_model_upload_layer_argument_t *)layer_body, ctx);
            break;
        default:
            assert(!&quot;Layer is not supported.&quot;);
    }

    if (cnt_layer_id != (ctx-&gt;layers_length - 1))
        ai_step(userdata);
    else
        kpu_kmodel_done(ctx);
    return 0;
}

int kpu_run_kmodel(kpu_model_context_t *ctx, const uint8_t *src, dmac_channel_number_t dma_ch, kpu_done_callback_t done_callback, void *userdata)
{
    ctx-&gt;dma_ch = dma_ch;
    ctx-&gt;done_callback = done_callback;
    ctx-&gt;userdata = userdata;
    ctx-&gt;current_layer = 0;
    ctx-&gt;current_body = ctx-&gt;body_start;

    kpu_kmodel_header_t *header = (kpu_kmodel_header_t *)ctx-&gt;model_buffer;
    kpu-&gt;interrupt_clear.reg = 7;
    kpu-&gt;fifo_threshold.data = (kpu_config_fifo_threshold_t)
    {
        .fifo_full_threshold = 10, .fifo_empty_threshold = 1
    };
    kpu-&gt;eight_bit_mode.data = (kpu_config_eight_bit_mode_t)
    {
        .eight_bit_mode = header-&gt;flags &amp; 1
    };
    kpu-&gt;interrupt_mask.data = (kpu_config_interrupt_t)
    {
        .calc_done_int = 1,
        .layer_cfg_almost_empty_int = 0,
        .layer_cfg_almost_full_int = 1
    };

    plic_irq_enable(IRQN_AI_INTERRUPT);
    plic_set_priority(IRQN_AI_INTERRUPT, 1);
    plic_irq_register(IRQN_AI_INTERRUPT, ai_step, ctx);

    const kpu_model_layer_header_t *first_layer_header = ctx-&gt;layer_headers;
    if (first_layer_header-&gt;type != KL_K210_CONV)
        return -1;
    const kpu_model_conv_layer_argument_t *first_layer = (const kpu_model_conv_layer_argument_t *)ctx-&gt;body_start;
    kpu_layer_argument_t layer_arg = *(volatile kpu_layer_argument_t *)(ctx-&gt;model_buffer + first_layer-&gt;layer_offset);

    if ((layer_arg.image_size.data.i_row_wid + 1) % 64 != 0)
    {
        kpu_kmodel_input_with_padding(&amp;layer_arg, src);
        ai_step_not_isr(ctx);
    }
    else
    {
        kpu_input_dma(&amp;layer_arg, src, ctx-&gt;dma_ch, ai_step, ctx);
    }

    return 0;
}
</pre>

  </div>
</article>

    <div class="blog-post-comments">
        <div id="disqus_thread">
            <noscript>加载评论需要在浏览器启用 JavaScript 脚本支持。</noscript>
        </div>
    </div>



        
          <div id="footer-post-container">
  <div id="footer-post">

    <div id="nav-footer" style="display: none">
      <ul>
         
          <li><a href="/">首页</a></li>
         
          <li><a href="/about/">关于</a></li>
         
          <li><a href="/archives/">归档</a></li>
         
          <li><a href="/projects_url">项目</a></li>
        
      </ul>
    </div>

    <div id="toc-footer" style="display: none">
      <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-简介"><span class="toc-number">1.</span> <span class="toc-text">1.简介</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-工作原理"><span class="toc-number">2.</span> <span class="toc-text">2.工作原理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-代码实现（加载-gencode-output-c文件）"><span class="toc-number">3.</span> <span class="toc-text">3.代码实现（加载 gencode_output.c文件）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-代码实现（加载-kmodel）"><span class="toc-number">4.</span> <span class="toc-text">4.代码实现（加载 .kmodel）</span></a></li></ol>
    </div>

    <div id="share-footer" style="display: none">
      <ul>
  <li><a class="icon" href="http://www.facebook.com/sharer.php?u=https://elfmedy.github.io/2019/04/18/kendryte-kpu/"><i class="fab fa-facebook fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://twitter.com/share?url=https://elfmedy.github.io/2019/04/18/kendryte-kpu/&text=kendryte kpu"><i class="fab fa-twitter fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.linkedin.com/shareArticle?url=https://elfmedy.github.io/2019/04/18/kendryte-kpu/&title=kendryte kpu"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://pinterest.com/pin/create/bookmarklet/?url=https://elfmedy.github.io/2019/04/18/kendryte-kpu/&is_video=false&description=kendryte kpu"><i class="fab fa-pinterest fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=kendryte kpu&body=Check out this article: https://elfmedy.github.io/2019/04/18/kendryte-kpu/"><i class="fas fa-envelope fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://getpocket.com/save?url=https://elfmedy.github.io/2019/04/18/kendryte-kpu/&title=kendryte kpu"><i class="fab fa-get-pocket fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://reddit.com/submit?url=https://elfmedy.github.io/2019/04/18/kendryte-kpu/&title=kendryte kpu"><i class="fab fa-reddit fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.stumbleupon.com/submit?url=https://elfmedy.github.io/2019/04/18/kendryte-kpu/&title=kendryte kpu"><i class="fab fa-stumbleupon fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://digg.com/submit?url=https://elfmedy.github.io/2019/04/18/kendryte-kpu/&title=kendryte kpu"><i class="fab fa-digg fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.tumblr.com/share/link?url=https://elfmedy.github.io/2019/04/18/kendryte-kpu/&name=kendryte kpu&description="><i class="fab fa-tumblr fa-lg" aria-hidden="true"></i></a></li>
</ul>

    </div>

    <div id="actions-footer">
        <a id="menu" class="icon" href="#" onclick="$('#nav-footer').toggle();return false;"><i class="fas fa-bars fa-lg" aria-hidden="true"></i> 菜单</a>
        <a id="toc" class="icon" href="#" onclick="$('#toc-footer').toggle();return false;"><i class="fas fa-list fa-lg" aria-hidden="true"></i> 目录</a>
        <a id="share" class="icon" href="#" onclick="$('#share-footer').toggle();return false;"><i class="fas fa-share-alt fa-lg" aria-hidden="true"></i> 分享</a>
        <a id="top" style="display:none" class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up fa-lg" aria-hidden="true"></i> 返回顶部</a>
    </div>

  </div>
</div>

        
        <footer id="footer">
  <div class="footer-left">
    Copyright &copy; 2020 elfmedy
  </div>
  <div class="footer-right">
    <nav>
      <ul>
         
          <li><a href="/">首页</a></li>
         
          <li><a href="/about/">关于</a></li>
         
          <li><a href="/archives/">归档</a></li>
         
          <li><a href="/projects_url">项目</a></li>
        
      </ul>
    </nav>
  </div>
</footer>

    </div>
    <!-- styles -->
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
<link rel="stylesheet" href="/lib/justified-gallery/css/justifiedGallery.min.css">

    <!-- jquery -->
<script src="/lib/jquery/jquery.min.js"></script>
<script src="/lib/justified-gallery/js/jquery.justifiedGallery.min.js"></script>
<script src="/js/main.js"></script>
<!-- search -->

<!-- Google Analytics -->

<!-- Baidu Analytics -->

<!-- Disqus Comments -->

    <script type="text/javascript">
        var disqus_shortname = 'elfemdy';

        (function(){
            var dsq = document.createElement('script');
            dsq.type = 'text/javascript';
            dsq.async = true;
            dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        }());
    </script>


</body>
</html>
