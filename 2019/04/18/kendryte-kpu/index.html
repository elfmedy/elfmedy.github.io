<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 3.9.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"elfmedy.github.io","root":"/","scheme":"Gemini","version":"7.7.2","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":false,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="1.简介KPU 是通用神经网络处理器，内置卷积、批归一化、激活、池化运算单元，可以对人脸或物体进行实时检测，具体特性如下：  支持主流训练框架按照特定限制规则训练出来的定点化模型 对网络层数无直接限制，支持每层卷积神经网络参数单独配置，包括输入输出通道数目、输入输出行宽列高 支持两种卷积内核 1x1 和 3x3 支持任意形式的激活函数 实时工作时最大支持神经网络参数大小为 5.5MiB 到 5.9">
<meta name="keywords" content="MCU,k210,KPU">
<meta property="og:type" content="article">
<meta property="og:title" content="kendryte kpu">
<meta property="og:url" content="https://elfmedy.github.io/2019/04/18/kendryte-kpu/index.html">
<meta property="og:site_name" content="elfmedy">
<meta property="og:description" content="1.简介KPU 是通用神经网络处理器，内置卷积、批归一化、激活、池化运算单元，可以对人脸或物体进行实时检测，具体特性如下：  支持主流训练框架按照特定限制规则训练出来的定点化模型 对网络层数无直接限制，支持每层卷积神经网络参数单独配置，包括输入输出通道数目、输入输出行宽列高 支持两种卷积内核 1x1 和 3x3 支持任意形式的激活函数 实时工作时最大支持神经网络参数大小为 5.5MiB 到 5.9">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="https://elfmedy.github.io/2019/04/18/kendryte-kpu/kpu1.png">
<meta property="og:image" content="https://elfmedy.github.io/2019/04/18/kendryte-kpu/kpu2.jpg">
<meta property="og:updated_time" content="2020-03-17T12:16:42.003Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="kendryte kpu">
<meta name="twitter:description" content="1.简介KPU 是通用神经网络处理器，内置卷积、批归一化、激活、池化运算单元，可以对人脸或物体进行实时检测，具体特性如下：  支持主流训练框架按照特定限制规则训练出来的定点化模型 对网络层数无直接限制，支持每层卷积神经网络参数单独配置，包括输入输出通道数目、输入输出行宽列高 支持两种卷积内核 1x1 和 3x3 支持任意形式的激活函数 实时工作时最大支持神经网络参数大小为 5.5MiB 到 5.9">
<meta name="twitter:image" content="https://elfmedy.github.io/2019/04/18/kendryte-kpu/kpu1.png">

<link rel="canonical" href="https://elfmedy.github.io/2019/04/18/kendryte-kpu/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true
  };
</script>

  <title>kendryte kpu | elfmedy</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">elfmedy</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档</a>

  </li>
  </ul>

</nav>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block " lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://elfmedy.github.io/2019/04/18/kendryte-kpu/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="elfmedy">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="elfmedy">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          kendryte kpu
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2019-04-18 22:11:41" itemprop="dateCreated datePublished" datetime="2019-04-18T22:11:41+08:00">2019-04-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-03-17 20:16:42" itemprop="dateModified" datetime="2020-03-17T20:16:42+08:00">2020-03-17</time>
              </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
      <span class="post-meta-item-text">Disqus：</span>
    
    <a title="disqus" href="/2019/04/18/kendryte-kpu/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2019/04/18/kendryte-kpu/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h2 id="1-简介"><a href="#1-简介" class="headerlink" title="1.简介"></a>1.简介</h2><p>KPU 是通用神经网络处理器，内置卷积、批归一化、激活、池化运算单元，可以对人脸或物体进行实时检测，具体特性如下：</p>
<ul>
<li>支持主流训练框架按照特定限制规则训练出来的定点化模型</li>
<li>对网络层数无直接限制，支持每层卷积神经网络参数单独配置，包括输入输出通道数目、输入输出行宽列高</li>
<li>支持两种卷积内核 1x1 和 3x3</li>
<li>支持任意形式的激活函数</li>
<li>实时工作时最大支持神经网络参数大小为 5.5MiB 到 5.9MiB</li>
<li>非实时工作时最大支持网络参数大小为（Flash 容量-软件体积）</li>
</ul>
<img src="/2019/04/18/kendryte-kpu/kpu1.png" title="kpu ram">
<ol>
<li>*非实时场合一般用于音频应用，这类应用一般不需要 33ms 内获得神经网络输出结果。</li>
<li>*Flash 大小可选择为：SPI NOR Flash（8MiB，16MiB，32MiB），SPI NAND Flash（64MiB，128MiB，256MiB），用户可根据需要选择合适的 Flash</li>
</ol>
<p>注意，KPU 只支持卷积相关的操作，所以其他的全连接什么的都是需要 CPU 来实现的，sdk 中也有相关的代码实现。</p>
<a id="more"></a>
<h2 id="2-工作原理"><a href="#2-工作原理" class="headerlink" title="2.工作原理"></a>2.工作原理</h2><img src="/2019/04/18/kendryte-kpu/kpu2.jpg" title="kpu">
<p>引用 &lt;<a href="https://bbs.sipeed.com/t/topic/502" target="_blank" rel="noopener">https://bbs.sipeed.com/t/topic/502</a>&gt;</p>
<h2 id="3-代码实现（加载-gencode-output-c文件）"><a href="#3-代码实现（加载-gencode-output-c文件）" class="headerlink" title="3.代码实现（加载 gencode_output.c文件）"></a>3.代码实现（加载 gencode_output.c文件）</h2><p>需要说明的是，官方目前的 sdk develop 分支中，已经不再采用 gencode_output.c 文件来初始化了，而是使用 .kmodel 模型来初始化 kpu，而且 sdk 的代码也做了不少修改，之前的这种方式的问题应该是只能实现卷积相关的操作，而采用 .kmodle 中的结构方式之后，方便使用 CPU 来实现其他的自定义层的操作。</p>
<p>通过 kendryte-model-compile 将模型转换为 k210 支持的形式，这里是 gencode_output.c 文件，其中包含了神经网络的层级结构的结构体和数组（模型的主要占用空间的就是各种权重参数的数组），还有针对这个模型的 kpu_task_init 函数实现。部分代码如下：</p>
<pre class="themepre">
<span class="themespan">// 这个是网络层次结构体，其中包含了各层的参数设置</span>
<span class="themespan">// 因为是数组，所有可以有很多层</span>
kpu_layer_argument_t la[] __attribute__((aligned(128))) = {
// 0
{
 .interrupt_enabe.data = {
  .int_en = 0,
  .ram_flag = 0,
  .full_add = 0,
  .depth_wise_layer = 0
 },
 .image_addr.data = {
  .image_src_addr = (uint64_t)0x0,
  .image_dst_addr = (uint64_t)0x6980
 },
 .image_channel_num.data = {
  .i_ch_num = 0x2,
  .o_ch_num = 0xf,
  .o_ch_num_coef = 0xf
 },
 .image_size.data = {
  .i_row_wid = 0x13f,
  .i_col_high = 0xef,
  .o_row_wid = 0x9f,
  .o_col_high = 0x77
 },
 .kernel_pool_type_cfg.data = {
  .kernel_type = 1,
  .pad_type = 0,
  .pool_type = 1,
  .first_stride = 0,
  .bypass_conv = 0,
  .load_para = 1,
  .dma_burst_size = 15,
  .pad_value = 0x0,
  .bwsx_base_addr = 0
 },
 .kernel_load_cfg.data = {
  .load_coor = 1,
  .load_time = 0,
  .para_size = 864,
  .para_start_addr = 0
 },
 .kernel_offset.data = {
  .coef_column_offset = 0,
  .coef_row_offset = 0
 },
 .kernel_calc_type_cfg.data = {
  .channel_switch_addr = 0x4b0,
  .row_switch_addr = 0x5,
  .coef_size = 0,
  .coef_group = 1,
  .load_act = 1,
  .active_addr = 0
 },
 .write_back_cfg.data = {
  .wb_channel_switch_addr = 0x168,
  .wb_row_switch_addr = 0x3,
  .wb_group = 1
 },
 .conv_value.data = {
  .shr_w = 0,
  .shr_x = 7,
  .arg_w = 0x0,
  .arg_x = 0xb8cb99
 },
 .conv_value2.data = {
  .arg_add = 0
 },
 .dma_parameter.data = {
  .send_data_out = 0,
  .channel_byte_num = 19199,
  .dma_total_byte = 307199
 }
},
// 1
{
 .interrupt_enabe.data = {
  .int_en = 0,

<span class="themespan">// kpu_task_init 函数</span>
<span class="themespan">// 除了赋值 layer 函数之外，还对 layer 里面的每一层里面的数据指针做了赋值（层结构体中的指针指向权重系数等）</span>
kpu_task_t* kpu_task_init(kpu_task_t* task){
 la[0].kernel_pool_type_cfg.data.bwsx_base_addr = (uint64_t)&amp;bwsx_base_addr_0;  <span class="themespan">// 这些都是指向系数的数组</span>
 la[0].kernel_calc_type_cfg.data.active_addr = (uint64_t)&amp;active_addr_0;
 la[0].kernel_load_cfg.data.para_start_addr = (uint64_t)&amp;para_start_addr_0; 
 la[1].kernel_pool_type_cfg.data.bwsx_base_addr = (uint64_t)&amp;bwsx_base_addr_1;
 la[1].kernel_calc_type_cfg.data.active_addr = (uint64_t)&amp;active_addr_1;
 la[1].kernel_load_cfg.data.para_start_addr = (uint64_t)&amp;para_start_addr_1; 
 la[2].kernel_pool_type_cfg.data.bwsx_base_addr = (uint64_t)&amp;bwsx_base_addr_2;
 la[2].kernel_calc_type_cfg.data.active_addr = (uint64_t)&amp;active_addr_2;
 la[2].kernel_load_cfg.data.para_start_addr = (uint64_t)&amp;para_start_addr_2; 
 la[3].kernel_pool_type_cfg.data.bwsx_base_addr = (uint64_t)&amp;bwsx_base_addr_3;
 la[3].kernel_calc_type_cfg.data.active_addr = (uint64_t)&amp;active_addr_3;
 la[3].kernel_load_cfg.data.para_start_addr = (uint64_t)&amp;para_start_addr_3; 
 la[4].kernel_pool_type_cfg.data.bwsx_base_addr = (uint64_t)&amp;bwsx_base_addr_4;
 la[4].kernel_calc_type_cfg.data.active_addr = (uint64_t)&amp;active_addr_4;
 la[4].kernel_load_cfg.data.para_start_addr = (uint64_t)&amp;para_start_addr_4; 
 la[5].kernel_pool_type_cfg.data.bwsx_base_addr = (uint64_t)&amp;bwsx_base_addr_5;
 la[5].kernel_calc_type_cfg.data.active_addr = (uint64_t)&amp;active_addr_5;
 la[5].kernel_load_cfg.data.para_start_addr = (uint64_t)&amp;para_start_addr_5; 
 la[6].kernel_pool_type_cfg.data.bwsx_base_addr = (uint64_t)&amp;bwsx_base_addr_6;
 la[6].kernel_calc_type_cfg.data.active_addr = (uint64_t)&amp;active_addr_6;
 la[6].kernel_load_cfg.data.para_start_addr = (uint64_t)&amp;para_start_addr_6; 
 la[7].kernel_pool_type_cfg.data.bwsx_base_addr = (uint64_t)&amp;bwsx_base_addr_7;
 la[7].kernel_calc_type_cfg.data.active_addr = (uint64_t)&amp;active_addr_7;
 la[7].kernel_load_cfg.data.para_start_addr = (uint64_t)&amp;para_start_addr_7; 
 la[8].kernel_pool_type_cfg.data.bwsx_base_addr = (uint64_t)&amp;bwsx_base_addr_8;
 la[8].kernel_calc_type_cfg.data.active_addr = (uint64_t)&amp;active_addr_8;
 la[8].kernel_load_cfg.data.para_start_addr = (uint64_t)&amp;para_start_addr_8; 
 la[9].kernel_pool_type_cfg.data.bwsx_base_addr = (uint64_t)&amp;bwsx_base_addr_9;
 la[9].kernel_calc_type_cfg.data.active_addr = (uint64_t)&amp;active_addr_9;
 la[9].kernel_load_cfg.data.para_start_addr = (uint64_t)&amp;para_start_addr_9; 
 la[10].kernel_pool_type_cfg.data.bwsx_base_addr = (uint64_t)&amp;bwsx_base_addr_10;
 la[10].kernel_calc_type_cfg.data.active_addr = (uint64_t)&amp;active_addr_10;
 la[10].kernel_load_cfg.data.para_start_addr = (uint64_t)&amp;para_start_addr_10; 
 la[11].kernel_pool_type_cfg.data.bwsx_base_addr = (uint64_t)&amp;bwsx_base_addr_11;
 la[11].kernel_calc_type_cfg.data.active_addr = (uint64_t)&amp;active_addr_11;
 la[11].kernel_load_cfg.data.para_start_addr = (uint64_t)&amp;para_start_addr_11; 
 la[12].kernel_pool_type_cfg.data.bwsx_base_addr = (uint64_t)&amp;bwsx_base_addr_12;
 la[12].kernel_calc_type_cfg.data.active_addr = (uint64_t)&amp;active_addr_12;
 la[12].kernel_load_cfg.data.para_start_addr = (uint64_t)&amp;para_start_addr_12; 
 la[13].kernel_pool_type_cfg.data.bwsx_base_addr = (uint64_t)&amp;bwsx_base_addr_13;
 la[13].kernel_calc_type_cfg.data.active_addr = (uint64_t)&amp;active_addr_13;
 la[13].kernel_load_cfg.data.para_start_addr = (uint64_t)&amp;para_start_addr_13; 
 la[14].kernel_pool_type_cfg.data.bwsx_base_addr = (uint64_t)&amp;bwsx_base_addr_14;
 la[14].kernel_calc_type_cfg.data.active_addr = (uint64_t)&amp;active_addr_14;
 la[14].kernel_load_cfg.data.para_start_addr = (uint64_t)&amp;para_start_addr_14; 
 la[15].kernel_pool_type_cfg.data.bwsx_base_addr = (uint64_t)&amp;bwsx_base_addr_15;
 la[15].kernel_calc_type_cfg.data.active_addr = (uint64_t)&amp;active_addr_15;
 la[15].kernel_load_cfg.data.para_start_addr = (uint64_t)&amp;para_start_addr_15;
 task-&gt;layers = la;
 task-&gt;layers_length = sizeof(la)/sizeof(la[0]);
 task-&gt;eight_bit_mode = 0;
 task-&gt;output_scale = 0.06265033647125842;
 task-&gt;output_bias = -14.453129768371582;
 return task;
}
</pre>

<p>至于 kpu 的运行，主要体现在 kpu_run 函数中，部分代码如下。先使用 DMA 将输入图像数据复制到 KPU 的处理的物理地址，然后根据 KPU 的中断将网络参数赋值给 KPU 的网络参数 fifo 寄存器（这里是每次写入一层网络参数）。在 KPU 计算完成之后触发 DMA 中断将数据拷贝到输出缓存。</p>
<p>KPU 的硬件寄存器接收的参数实际就是一组 uint64 的数据，每次循环写入 kpu-&gt;layer_argument_fifo，这个是固定的。而 task 结构体恰好设置为和 KPU 的硬件寄存器接收的数据格式一致。</p>
<pre class="themepre">
<span class="themespan">// kpu 中断处理函数</span>
int kpu_continue(void* _task)
{
    kpu_task_t* task = (kpu_task_t*)_task;
    int layer_burst_size = 1;

    kpu-&gt;interrupt_clear.data = (kpu_config_interrupt_t)
    {
        .calc_done_int=1,
        .layer_cfg_almost_empty_int=1,
        .layer_cfg_almost_full_int=1
    };

    if(task-&gt;remain_layers_length == 0)
    {
        return 0;
    }
    if(task-&gt;remain_layers_length &lt;= layer_burst_size)
    {
        for(uint32_t i=0; i&lt;task-&gt;remain_layers_length; i++)
        {
            <span class="themespan">// 这里是重复写 fifo 寄存器，直到所有参数写完</span>
            kpu-&gt;layer_argument_fifo = task-&gt;remain_layers[i].interrupt_enabe.reg;
            kpu-&gt;layer_argument_fifo = task-&gt;remain_layers[i].image_addr.reg;
            kpu-&gt;layer_argument_fifo = task-&gt;remain_layers[i].image_channel_num.reg;
            kpu-&gt;layer_argument_fifo = task-&gt;remain_layers[i].image_size.reg;
            kpu-&gt;layer_argument_fifo = task-&gt;remain_layers[i].kernel_pool_type_cfg.reg;
            kpu-&gt;layer_argument_fifo = task-&gt;remain_layers[i].kernel_load_cfg.reg;
            kpu-&gt;layer_argument_fifo = task-&gt;remain_layers[i].kernel_offset.reg;
            kpu-&gt;layer_argument_fifo = task-&gt;remain_layers[i].kernel_calc_type_cfg.reg;
            kpu-&gt;layer_argument_fifo = task-&gt;remain_layers[i].write_back_cfg.reg;
            kpu-&gt;layer_argument_fifo = task-&gt;remain_layers[i].conv_value.reg;
            kpu-&gt;layer_argument_fifo = task-&gt;remain_layers[i].conv_value2.reg;
            kpu-&gt;layer_argument_fifo = task-&gt;remain_layers[i].dma_parameter.reg;
        }
        task-&gt;remain_layers_length = 0;
    }
    else
    {
        <span class="themespan">// 在中断处理函数中依次将每一层的网络参数赋值给 kpu 寄存器的 layer_argument_fifo 中</span>
        for(uint32_t i=0; i&lt;layer_burst_size; i++)
        {
            kpu-&gt;layer_argument_fifo = task-&gt;remain_layers[i].interrupt_enabe.reg;
            kpu-&gt;layer_argument_fifo = task-&gt;remain_layers[i].image_addr.reg;
            kpu-&gt;layer_argument_fifo = task-&gt;remain_layers[i].image_channel_num.reg;
            kpu-&gt;layer_argument_fifo = task-&gt;remain_layers[i].image_size.reg;
            kpu-&gt;layer_argument_fifo = task-&gt;remain_layers[i].kernel_pool_type_cfg.reg;
            kpu-&gt;layer_argument_fifo = task-&gt;remain_layers[i].kernel_load_cfg.reg;
            kpu-&gt;layer_argument_fifo = task-&gt;remain_layers[i].kernel_offset.reg;
            kpu-&gt;layer_argument_fifo = task-&gt;remain_layers[i].kernel_calc_type_cfg.reg;
            kpu-&gt;layer_argument_fifo = task-&gt;remain_layers[i].write_back_cfg.reg;
            kpu-&gt;layer_argument_fifo = task-&gt;remain_layers[i].conv_value.reg;
            kpu-&gt;layer_argument_fifo = task-&gt;remain_layers[i].conv_value2.reg;
            kpu-&gt;layer_argument_fifo = task-&gt;remain_layers[i].dma_parameter.reg;
        }
        task-&gt;remain_layers += layer_burst_size;
        task-&gt;remain_layers_length -= layer_burst_size;
    }
    printf(&quot;kpu_continue. remain_layers_length=%d\n&quot;, task-&gt;remain_layers_length);

    return 0;
}

<span class="themespan">// 将数据放入 dma 处理地址地址之后的回调函数</span>
static int kpu_run_dma_input_done_push_layers(void* _task)
{
    printf(&quot;kpu_run_dma_input_done_push_layers. input dma over cb\n&quot;);

    kpu_task_t* task = (kpu_task_t*)_task;
    kpu-&gt;interrupt_clear.reg = 7;
    dmac-&gt;channel[task-&gt;dma_ch].intclear = 0xFFFFFFFF;
    kpu-&gt;fifo_threshold.data = (kpu_config_fifo_threshold_t)
    {
        .fifo_full_threshold = 10, .fifo_empty_threshold=1
    };
    kpu-&gt;eight_bit_mode.data = (kpu_config_eight_bit_mode_t)
    {
        .eight_bit_mode=task-&gt;eight_bit_mode
    };

    kpu_layer_argument_t* last_layer = &amp;task-&gt;layers[task-&gt;layers_length-1];

    <span class="themespan">// 这里只是注册 kpu 运算结束之后的 dma 操作。在运算结束之后将输出数据复制到输出缓冲区中</span>
    <span class="themespan">// 这个函数中会调用 sysctl_dma_select(dma_ch, SYSCTL_DMA_SELECT_AI_RX_REQ); 这个选择的中断源应该就是 kpu 计算结束之后的 RX 请求</span>
    kpu_run_dma_output(task-&gt;dma_ch, task-&gt;dst, last_layer-&gt;dma_parameter.data.dma_total_byte+1, kpu_run_all_done, task);

    kpu-&gt;interrupt_mask.data = (kpu_config_interrupt_t)
    {
        .calc_done_int=0,
        .layer_cfg_almost_empty_int=0,
        .layer_cfg_almost_full_int=1
    };

    <span class="themespan">// 这里开始会放入一次，后面调用 kpu_continue 就都是 kpu 的中断本身在调用了</span>
    kpu_continue(task);
    return 0;
}

<span class="themespan">// kpu_run 函数</span>
int kpu_run(kpu_task_t* v_task, dmac_channel_number_t dma_ch, const void *src, void* dest, plic_irq_callback_t callback)
{
    if(atomic_cas(&amp;g_kpu_context.kpu_status, 0, 1))
        return -1;

    memcpy((void *)&amp;g_kpu_context.kpu_task, v_task, sizeof(kpu_task_t));
    kpu_task_t *task = (kpu_task_t *)&amp;g_kpu_context.kpu_task;

    kpu_layer_argument_t* last_layer = &amp;task-&gt;layers[task-&gt;layers_length-1];

    uint64_t output_size = last_layer-&gt;dma_parameter.data.dma_total_byte+1;
    uint32_t size = output_size;

    last_layer-&gt;dma_parameter.data.send_data_out = 1;
    last_layer-&gt;interrupt_enabe.data.int_en = 1;

    task-&gt;dma_ch = dma_ch;
    task-&gt;dst = dest;
    task-&gt;dst_length = output_size;
    task-&gt;cb = callback;
    task-&gt;remain_layers_length = task-&gt;layers_length;
    task-&gt;remain_layers = task-&gt;layers;

    <span class="themespan">// 使能 kpu 的中断处理函数</span>
    plic_irq_enable(IRQN_AI_INTERRUPT);
    plic_set_priority(IRQN_AI_INTERRUPT, 1);
    plic_irq_register(IRQN_AI_INTERRUPT, kpu_continue, task);

    <span class="themespan">// 将输入图像数据放入 kpu 的处理地址，src-&gt;AI_IO_BASE_ADDR，这个是立即执行的</span>
    kpu_run_dma_input(dma_ch, src, kpu_run_dma_input_done_push_layers, task);

    return 0;
}
</pre>

<h2 id="4-代码实现（加载-kmodel）"><a href="#4-代码实现（加载-kmodel）" class="headerlink" title="4.代码实现（加载 .kmodel）"></a>4.代码实现（加载 .kmodel）</h2><p>.kmodle 模型可以存放在 flash 中，或者直接使用 sdk 提供的宏来包含模型，也就是将 .kmodel 当做数组。加载模型也就是赋值一些 ctx 的参数。.kmodel 模型是使用 nncase 生成的。</p>
<pre class="themepre">
int kpu_load_kmodel(kpu_model_context_t *ctx, const uint8_t *buffer)
{
    uintptr_t base_addr = (uintptr_t)buffer;
    const kpu_kmodel_header_t *header = (const kpu_kmodel_header_t *)buffer;

    if (header-&gt;version == 3 &amp;&amp; header-&gt;arch == 0)
    {
        ctx-&gt;model_buffer = buffer;
        ctx-&gt;output_count = header-&gt;output_count;
        ctx-&gt;outputs = (const kpu_model_output_t *)(base_addr + sizeof(kpu_kmodel_header_t));
        ctx-&gt;layer_headers = (const kpu_model_layer_header_t *)((uintptr_t)ctx-&gt;outputs + sizeof(kpu_model_output_t) * ctx-&gt;output_count);
        ctx-&gt;layers_length = header-&gt;layers_length;
        ctx-&gt;body_start = (const uint8_t *)((uintptr_t)ctx-&gt;layer_headers + sizeof(kpu_model_layer_header_t) * header-&gt;layers_length);
        ctx-&gt;main_buffer = (uint8_t *)malloc(header-&gt;main_mem_usage);
        if (!ctx-&gt;main_buffer)
            return -1;
    }
    else
    {
        return -1;
    }

    return 0;
}
</pre>

<p>模型运行的话基本和前面使用 gencode_output.c 文件的方式类似，都是一层一层加载网络，不过多出来的地方是增加了使用 CPU 来实现一些操作，下面的一些 cnt_layer_header-&gt;type 判断中，只有卷积相关是直接 DMA 拷贝实现的，其他的都是 CPU 直接算。</p>
<pre class="themepre">
static int ai_step(void *userdata)
{
    kpu_model_context_t *ctx = (kpu_model_context_t *)userdata;

    uint32_t cnt_layer_id = ctx-&gt;current_layer++;
    const uint8_t *layer_body = ctx-&gt;current_body;
    const kpu_model_layer_header_t *cnt_layer_header = ctx-&gt;layer_headers + cnt_layer_id;
    ctx-&gt;current_body += cnt_layer_header-&gt;body_size;

    <span class="themespan">// 除了卷积相关，都是 CPU 实现的</span>
    switch (cnt_layer_header-&gt;type)
    {
        case KL_ADD:
            kpu_kmodel_add((const kpu_model_add_layer_argument_t *)layer_body, ctx);
            break;
        case KL_QUANTIZED_ADD:
            kpu_quantized_add((const kpu_model_quant_add_layer_argument_t *)layer_body, ctx);
            break;
        case KL_GLOBAL_AVERAGE_POOL2D:
            kpu_global_average_pool2d((const kpu_model_gap2d_layer_argument_t *)layer_body, ctx);
            break;
        case KL_QUANTIZED_MAX_POOL2D:
            kpu_quantized_max_pool2d((const kpu_model_quant_max_pool2d_layer_argument_t *)layer_body, ctx);
            break;
        case KL_AVERAGE_POOL2D:
            kpu_average_pool2d((const kpu_model_ave_pool2d_layer_argument_t *)layer_body, ctx);
            break;
        case KL_QUANTIZE:
            kpu_quantize((const kpu_model_quantize_layer_argument_t *)layer_body, ctx);
            break;
        case KL_DEQUANTIZE:
            kpu_kmodel_dequantize((const kpu_model_dequantize_layer_argument_t *)layer_body, ctx);
            break;
        case KL_REQUANTIZE:
            kpu_requantize((const kpu_model_requantize_layer_argument_t *)layer_body, ctx);
            break;
        case KL_L2_NORMALIZATION:
            kpu_l2_normalization((const kpu_model_l2_norm_layer_argument_t *)layer_body, ctx);
            break;
        case KL_SOFTMAX:
            kpu_softmax((const kpu_model_softmax_layer_argument_t *)layer_body, ctx);
            break;
        case KL_CONCAT:
        case KL_QUANTIZED_CONCAT:
            kpu_concat((const kpu_model_concat_layer_argument_t *)layer_body, ctx);
            break;
        case KL_FULLY_CONNECTED:
            kpu_kmodel_fully_connected((const kpu_model_fully_connected_layer_argument_t *)layer_body, ctx);
            break;
        case KL_TENSORFLOW_FLATTEN:
            kpu_tf_flatten((const kpu_model_tf_flatten_layer_argument_t *)layer_body, ctx);
            break;
        case KL_K210_CONV:
            kpu_conv((const kpu_model_conv_layer_argument_t *)layer_body, ctx);
            return 0;
        case KL_K210_ADD_PADDING:
            kpu_add_padding((const kpu_model_add_padding_layer_argument_t *)layer_body, ctx);
            break;
        case KL_K210_REMOVE_PADDING:
            kpu_remove_padding((const kpu_model_remove_padding_layer_argument_t *)layer_body, ctx);
            break;
        case KL_K210_UPLOAD:
            kpu_upload((const kpu_model_upload_layer_argument_t *)layer_body, ctx);
            break;
        default:
            assert(!&quot;Layer is not supported.&quot;);
    }

    if (cnt_layer_id != (ctx-&gt;layers_length - 1))
        ai_step(userdata);
    else
        kpu_kmodel_done(ctx);
    return 0;
}

int kpu_run_kmodel(kpu_model_context_t *ctx, const uint8_t *src, dmac_channel_number_t dma_ch, kpu_done_callback_t done_callback, void *userdata)
{
    ctx-&gt;dma_ch = dma_ch;
    ctx-&gt;done_callback = done_callback;
    ctx-&gt;userdata = userdata;
    ctx-&gt;current_layer = 0;
    ctx-&gt;current_body = ctx-&gt;body_start;

    kpu_kmodel_header_t *header = (kpu_kmodel_header_t *)ctx-&gt;model_buffer;
    kpu-&gt;interrupt_clear.reg = 7;
    kpu-&gt;fifo_threshold.data = (kpu_config_fifo_threshold_t)
    {
        .fifo_full_threshold = 10, .fifo_empty_threshold = 1
    };
    kpu-&gt;eight_bit_mode.data = (kpu_config_eight_bit_mode_t)
    {
        .eight_bit_mode = header-&gt;flags &amp; 1
    };
    kpu-&gt;interrupt_mask.data = (kpu_config_interrupt_t)
    {
        .calc_done_int = 1,
        .layer_cfg_almost_empty_int = 0,
        .layer_cfg_almost_full_int = 1
    };

    plic_irq_enable(IRQN_AI_INTERRUPT);
    plic_set_priority(IRQN_AI_INTERRUPT, 1);
    plic_irq_register(IRQN_AI_INTERRUPT, ai_step, ctx);

    const kpu_model_layer_header_t *first_layer_header = ctx-&gt;layer_headers;
    if (first_layer_header-&gt;type != KL_K210_CONV)
        return -1;
    const kpu_model_conv_layer_argument_t *first_layer = (const kpu_model_conv_layer_argument_t *)ctx-&gt;body_start;
    kpu_layer_argument_t layer_arg = *(volatile kpu_layer_argument_t *)(ctx-&gt;model_buffer + first_layer-&gt;layer_offset);

    if ((layer_arg.image_size.data.i_row_wid + 1) % 64 != 0)
    {
        kpu_kmodel_input_with_padding(&amp;layer_arg, src);
        ai_step_not_isr(ctx);
    }
    else
    {
        kpu_input_dma(&amp;layer_arg, src, ctx-&gt;dma_ch, ai_step, ctx);
    }

    return 0;
}
</pre>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/MCU/" rel="tag"># MCU</a>
              <a href="/tags/k210/" rel="tag"># k210</a>
              <a href="/tags/KPU/" rel="tag"># KPU</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2019/04/16/使用 hexo 搭建博客/" rel="prev" title="使用 hexo 搭建博客">
      <i class="fa fa-chevron-left"></i> 使用 hexo 搭建博客
    </a></div>
      <div class="post-nav-item"></div>
    </div>
      </footer>
    
  </article>
  
  
  

  </div>


          </div>
          
    
  <div class="comments">
    <div id="disqus_thread">
      <noscript>Please enable JavaScript to view the comments powered by Disqus.</noscript>
    </div>
  </div>
  

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-简介"><span class="nav-text">1.简介</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-工作原理"><span class="nav-text">2.工作原理</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-代码实现（加载-gencode-output-c文件）"><span class="nav-text">3.代码实现（加载 gencode_output.c文件）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-代码实现（加载-kmodel）"><span class="nav-text">4.代码实现（加载 .kmodel）</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">elfmedy</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">2</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">4</span>
        <span class="site-state-item-name">标签</span>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/elfmedy" title="GitHub → https://github.com/elfmedy" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:zzmmdd@foxmail.com" title="E-Mail → mailto:zzmmdd@foxmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">elfmedy</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动
  </div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">主题 – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
<script src="/js/utils.js"></script>
<script src="/js/schemes/pisces.js"></script>
<script src="/js/next-boot.js"></script>



  















  

  

<script>
  function loadCount() {
    var d = document, s = d.createElement('script');
    s.src = 'https://elfemdy.disqus.com/count.js';
    s.id = 'dsq-count-scr';
    (d.head || d.body).appendChild(s);
  }
  // defer loading until the whole page loading is completed
  window.addEventListener('load', loadCount, false);
</script>
<script>
  var disqus_config = function() {
    this.page.url = "https://elfmedy.github.io/2019/04/18/kendryte-kpu/";
    this.page.identifier = "2019/04/18/kendryte-kpu/";
    this.page.title = "kendryte kpu";
    };
  NexT.utils.loadComments(document.querySelector('#disqus_thread'), () => {
    if (window.DISQUS) {
      DISQUS.reset({
        reload: true,
        config: disqus_config
      });
    } else {
      var d = document, s = d.createElement('script');
      s.src = 'https://elfemdy.disqus.com/embed.js';
      s.setAttribute('data-timestamp', '' + +new Date());
      (d.head || d.body).appendChild(s);
    }
  });
</script>

</body>
</html>
